## 第一课程笔记
https://blog.csdn.net/gaxzj/article/details/135415566?spm=1001.2014.3001.5501



## 第二课笔记
https://blog.csdn.net/gaxzj/article/details/135441527?spm=1001.2014.3001.5501

## 第二课作业
### 基础作业
使用 InternLM-Chat-7B 模型生成 300 字的小故事
![interlm_300](https://github.com/pranerd/internlm-campus/assets/19358928/37f78974-3d57-40d7-8c79-414cdcdff5cf)
熟悉 hugging face 下载功能，使用 huggingface_hub python 包，下载 InternLM-20B 的 config.json 文件到本地（需截图下载过程）
![interlm_hugging_download](https://github.com/pranerd/internlm-campus/assets/19358928/86456fc0-81f3-4aa4-b035-a90cc3907c69)
### 进阶作业
完成浦语·灵笔的图文理解及创作部署
![interlm_xcomposer_1](https://github.com/pranerd/internlm-campus/assets/19358928/9ded5171-9ff3-4066-8e8d-e7fd01a9232a)
![interlm_xcomposer_2](https://github.com/pranerd/internlm-campus/assets/19358928/07bacc4d-bdd5-4ba9-a888-d59f79c39239)
完成 Lagent 工具调用 Demo 创作部署
![interlm_lagent_1](https://github.com/pranerd/internlm-campus/assets/19358928/5d9cd2d0-7948-440b-82c1-e9230147d62f)
![interlm_lagent_2](https://github.com/pranerd/internlm-campus/assets/19358928/802cd363-3d8a-4c43-bf96-313a24a25a2a)
![interlm_lagent_3](https://github.com/pranerd/internlm-campus/assets/19358928/78aa3ee8-60f0-4d1e-9f03-ea678b2e10ea)


## 第三课笔记
https://blog.csdn.net/gaxzj/article/details/135462859?spm=1001.2014.3001.5501
## 第三课作业
### 基础作业
![interlm_langchain_1](https://github.com/pranerd/internlm-campus/assets/19358928/580068a8-b85d-4b5a-8ea6-8a6f5cebee91)
![interlm_langchain_2](https://github.com/pranerd/internlm-campus/assets/19358928/b4a94c7c-d95b-481c-a3e3-c8cc7db37133)
### 进阶作业
构建一个python的知识库
![interlm_langchain_python_1](https://github.com/pranerd/internlm-campus/assets/19358928/a120f5e6-4465-4ede-b283-b14fa8dc6d59)
![interlm_langchain_python_2](https://github.com/pranerd/internlm-campus/assets/19358928/911e51b7-6867-4c2b-85e1-3e443b4f57f0)

## 第四课笔记
https://blog.csdn.net/gaxzj/article/details/135586441?spm=1001.2014.3001.5501
## 第四课作业
### 基础作业
微调下的输出：
![internlm_xtuner_me](https://github.com/pranerd/internlm-campus/assets/19358928/46053c69-0b6b-4bd2-b561-515f46eb678e)
### 进阶作业
上传微调模型到openxlab上面
https://openxlab.org.cn/models/detail/pranerd/xtuner_lesson3_model
## 第五课笔记
https://blog.csdn.net/gaxzj/article/details/135707056?spm=1001.2014.3001.5501
## 第五课作业
### 基础作业
基础作业：
使用本地对话：
![internlm_lmdeploy_1](https://github.com/InternLM/tutorial/assets/19358928/dd993453-680d-48a7-aa6a-451ebc169a83)
使用API服务：
![internlm_lmdeploy_2](https://github.com/InternLM/tutorial/assets/19358928/c8e62260-c643-47db-8a66-090fced788ef)
![internlm_lmdeploy_3](https://github.com/InternLM/tutorial/assets/19358928/64549927-ca0f-4274-9590-65ae9fc0e191)
![internlm_lmdeploy_4](https://github.com/InternLM/tutorial/assets/19358928/9f8897d5-022b-4307-9944-e84daa23b54f)
使用gradio
gradio想到相当于client，有两种处理方式
这里直接启动作为前端的 Gradio
###  Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client
```
lmdeploy serve gradio http://0.0.0.0:23333 \
	--server_name 0.0.0.0 \
	--server_port 6006 \
	--restful_api True
```
![internlm_lmdeploy_7](https://github.com/InternLM/tutorial/assets/19358928/b596a73d-7d98-47c3-961f-255ce093a48a)
![internlm_lmdeploy_8](https://github.com/InternLM/tutorial/assets/19358928/6515a190-29d4-4e90-aa4c-b20a858a41d3)

也可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。具体命令为：
`lmdeploy serve gradio ./workspace`
![internlm_lmdeploy_5](https://github.com/InternLM/tutorial/assets/19358928/e224b1d6-15fb-4ffa-8acd-9500f2d5982a)
![internlm_lmdeploy_6](https://github.com/InternLM/tutorial/assets/19358928/0721a1f6-61cd-45fd-acf4-21c651d9e55e)
## 第六课笔记
https://blog.csdn.net/gaxzj/article/details/135707056?spm=1001.2014.3001.5501
## 第六课作业
### 基础作业
使用openCompass测试InternLM2-Chat-7B在C-Eval数据集上面的效果
运行命令

```
python run.py --datasets ceval_gen \
--hf-path /root/share/model_repos/internlm2-chat-7b/ \
--tokenizer-kwargs padding_side='left' truncation='left' \
trust_remote_code=True --model-kwargs trust_remote_code=True \
device_map='auto' --max-seq-len 1024 --max-out-len 16 --batch-size 2 --num-gpus 1
```
运行效果
![internlm_opencompass_2](https://github.com/InternLM/tutorial/assets/19358928/b3e3e419-eb59-4005-875a-aa6d9b9a5b36)
### 进阶作业
首先安装lmdeploy0.2.0,使用命令 `pip install lmdeploy==0.2.0`
接下来使用lmdeploy将模型进行转换
`lmdeploy convert internlm2-chat-7b /root/share/model_repos/internlm2-chat-7b --dst-path /root/model/internlm7b_lmdeploy2.0/`
#新建eval_internlm2-7b-deploy2.0.py
该脚本中的内容如下：
```
from mmengine.config import read_base
from opencompass.models.turbomind import TurboMindModel

with read_base():
    # choose a list of datasets
    from .datasets.ceval.ceval_gen_5f30c7 import ceval_datasets
    # and output the results in a choosen format
    from .summarizers.medium import summarizer


datasets = sum((v for k, v in locals().items() if k.endswith('_datasets')), [])

internlm_meta_template = dict(round=[
    dict(role='HUMAN', begin='<|User|>:', end='\n'),
    dict(role='BOT', begin='<|Bot|>:', end='<eoa>\n', generate=True),
],
                              eos_token_id=103028)

# config for internlm-chat-7b
internlm2_chat_7b = dict(
    type=TurboMindModel,
    abbr='internlm2-chat-7b-turbomind',
    path='/root/model/internlm7b_lmdeploy2.0/',
    engine_config=dict(session_len=2048,
                       max_batch_size=32,
                       rope_scaling_factor=1.0),
    gen_config=dict(top_k=1,
                    top_p=0.8,
                    temperature=1.0,
                    max_new_tokens=100),
    max_out_len=100,
    max_seq_len=2048,
    batch_size=32,
    concurrency=32,
    meta_template=internlm_meta_template,
    run_cfg=dict(num_gpus=1, num_procs=1),
)


models = [internlm2_chat_7b]
```
接下来运行命令
```
python run.py configs/eval_internlm2-7b-deploy2.0.py --debug --num-gpus 1
```
就可以出来结果了
![internlm_opencompass_3](https://github.com/InternLM/tutorial/assets/19358928/5033a9ed-e9d4-439c-91fb-286e7b67679b)

